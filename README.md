# GPT-by-Hand

This repository contains a tested, documented, and modular implementation of a GPT language model, based on Andrej Karpathy's [nanoGPT](https://github.com/karpathy/nanoGPT).

The project is designed to deepen the understanding of transformer-based language models. It combines a well-documented codebase with **model sizes small enough** that the calculations can be performed by hand, making it an ideal tool for learning and development.


## Small Model Sizes

TODO


## Code Structure

```
gpt-by-hand/
├── model.py             # GPT model architecture
├── model_config.py      # Model configuration class
├── model_test.py        # Tests for model components
├── train.py             # Training loop and logic
├── train_config.py      # Training configuration class
├── train_test.py        # Tests for training components
└── README.md            # Project documentation
```

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Acknowledgments
* Based on Andrej Karpathy's [nanoGPT](https://github.com/karpathy/nanoGPT)
* Inspired by the architecture of the original GPT models from OpenAI
